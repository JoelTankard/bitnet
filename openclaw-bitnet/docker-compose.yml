version: '3.8'

# Model switching: Run ./scripts/switch-model.sh [2b|large|3b|8b]
# This creates a .env file that overrides the defaults below

services:
  # BitNet Inference Server
  # Runs the 1-bit LLM with OpenAI-compatible API
  bitnet-server:
    build:
      context: ..
      dockerfile: openclaw-bitnet/docker/Dockerfile.bitnet
    container_name: bitnet-server
    restart: unless-stopped
    volumes:
      - bitnet-models:/models
      - ./models:/models/custom:ro
    environment:
      - BITNET_MODEL=${BITNET_MODEL:-/models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf}
      - BITNET_THREADS=${BITNET_THREADS:-4}
      - BITNET_CTX_SIZE=${BITNET_CTX_SIZE:-4096}
      - BITNET_PORT=8080
      - BITNET_HOST=0.0.0.0
    networks:
      - openclaw-network
    # Only expose to localhost for debugging (optional)
    # ports:
    #   - "127.0.0.1:8080:8080"
    deploy:
      resources:
        limits:
          memory: ${BITNET_MEMORY_LIMIT:-8G}
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  # OpenClaw Gateway
  # Personal AI assistant using BitNet as the LLM backend
  openclaw:
    build:
      context: .
      dockerfile: docker/Dockerfile.openclaw
    container_name: openclaw
    restart: unless-stopped
    depends_on:
      bitnet-server:
        condition: service_healthy
    volumes:
      - openclaw-data:/home/openclaw/.openclaw
      - ./config:/home/openclaw/.openclaw/config:ro
    environment:
      - BITNET_API_URL=http://bitnet-server:8080
      - NODE_ENV=production
    networks:
      - openclaw-network
    ports:
      # Expose gateway on localhost only for security
      - "127.0.0.1:18789:18789"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18789/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Model Downloader utility (run manually)
  model-downloader:
    image: python:3.11-slim
    container_name: model-downloader
    volumes:
      - bitnet-models:/models
    working_dir: /models
    entrypoint: ["sh", "-c"]
    command: ["pip install huggingface_hub && echo 'Ready to download models. Use: huggingface-cli download <model-name> --local-dir /models/<name>'"]
    profiles:
      - tools

networks:
  openclaw-network:
    driver: bridge
    internal: false

volumes:
  bitnet-models:
    name: openclaw-bitnet-models
  openclaw-data:
    name: openclaw-data
